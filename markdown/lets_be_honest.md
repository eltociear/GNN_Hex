# Let's be honest about the state of our results
* I did some more tests on GNN and the CNN architecture from Gao, and also the supervised learning advantage of GNN does not hold up in this scenario.
* Let's face it. We do not have any evidence to suggest that it is in any way advantageous to use GNN over the Gao CNN in Hex.
* I've been facing somewhat of a writing blockade the last weeks, trying to portray GNNs in a better light than what our evidence suggests.
* I don't think a conference like ICLR won't accept papers of the nature *We tried this new thing nobody cared about previously and it didn't work*
    + However, this is the only kind of paper we can reasonably write right now.
* I am sorry for setting different expectations for this project, but this has to be a common thing in the research industry.
    + Sometimes you think that your method works great, but then you do new experiments and find that your previous results were confounded (in my case, bad baseline) and your new evidence does not provide any real advantage for your method.
    + The scientific procedure can't be here to write around that new evidence to keep portraying the method in a light that makes it get accepted into a conference.
* Suggestion that would acutally help me around my writing blockade is to change the title to something like *GNNs do not beat CNNs in Hex despite advantages on long range dependencies*, make it a pure arxiv paper and include results like the one showing that the Gao CNN cushes the GNN on every board size after training on one.
* I know that, as a phd student, you face somewhat different pressure to publish than me, but I can not deliver you any results that would make a major conference publication.
* I would be kind of anxious about meeting with Kristian at that state of the project, I don't think he'll be too happy about the state of things.

![](/images/oh_well.svg)
![](/images/imitate_cnn_better.png)
![](/images/imitate_cnn_better2.png)
