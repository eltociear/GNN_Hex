# More open questions
+ Alpha-zero starts with a temperature 1 in the beginning of the game and then drops to zero after n moves. This makes sense, because exploration is more valuable in the beginning. However, from the paper and some reference implementations it seems like the temperature is also variied for the training targets. E.g. for the first n moves, the network is supposed to predict a distribution and for later moves is is supposed to predict 100% for the best move. This sounds like it would make training unstable. Why not use some fixed temperature for computing the training targets and only switch up the temperature for self-play?
+ The paper is a little unclear about which transitions should be used for training (And when should old transitions be thrown out). Is it smart to go with replay buffers similarly to DQN, or should I aim to only use transitions of the most recent agent?
